{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: ws01ent\n",
      "Azure region: westus2\n",
      "Subscription id: 0e9bace8-7a81-4922-83b5-d995ff706507\n",
      "Resource group: azureml\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### Create or Attach existing compute resource for Python steps\n",
    "By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this tutorial, you create Azure Machine Learning Compute as your training environment. The code below creates the compute clusters for you if they don't already exist in your workspace.\n",
    "\n",
    "**Creation of compute takes approximately 5 minutes. If the AmlCompute with that name is already in your workspace the code will skip the creation process.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found compute target. just use it. worker-cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"worker-cpu\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D12_V2\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core import Keyvault\n",
    "import os\n",
    "\n",
    "keyvault = ws.get_default_keyvault()\n",
    "keyvault.set_secret(name=\"servicebusconstr\", value = 'YOUR_SERVICE_BUS_CONNECTION_STRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This is to mount a BLob datastore\n",
    "from azureml.core import Datastore\n",
    "\n",
    "key =keyvault.get_secret('adlsgen6key')\n",
    "account_name = 'adlsdatalakegen6'\n",
    "datastore_sourcefiles= Datastore.register_azure_blob_container(workspace=ws, datastore_name = 'adlsgen6landing', \n",
    "                                                     container_name='landing',\n",
    "                                                     account_name= account_name, account_key=key,create_if_not_exists=False)\n",
    "\n",
    "datastore_preprocess= Datastore.register_azure_blob_container(workspace=ws, datastore_name = 'adlsgen6process', \n",
    "                                                     container_name='process',\n",
    "                                                     account_name= account_name, account_key=key,create_if_not_exists=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate/Output Data\n",
    "Intermediate data (or output of a Step) is represented by [PipelineData](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinedata?view=azure-ml-py) object. PipelineData can be produced by one step and consumed in another step by providing the PipelineData object as an output of one step and the input of one or more steps.\n",
    "\n",
    "**Constructing PipelineData**\n",
    "- name: [Required] Name of the data item within the pipeline graph\n",
    "- datastore_name: Name of the Datastore to write this output to\n",
    "- output_name: Name of the output\n",
    "- output_mode: Specifies \"upload\" or \"mount\" modes for producing output (default: mount)\n",
    "- output_path_on_compute: For \"upload\" mode, the path to which the module writes this output during execution\n",
    "- output_overwrite: Flag to overwrite pre-existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline, PipelineData,PipelineParameter\n",
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "\n",
    "\n",
    "scripts_folder = \"scripts\"\n",
    "#This is the source directory for the landing folder. It's used to read from files from\n",
    "datastore_sourcefiles_dir = Dataset.File.from_files((datastore_sourcefiles, \"/\")).as_named_input(\"datastore_sourcefiles_dir\")\n",
    "\n",
    "#This is to output the mapping file (mapping.csv)\n",
    "mapping_output_dir = PipelineData(name=\"grouping_output\", \n",
    "                          datastore=datastore_sourcefiles, \n",
    "                          output_path_on_compute=\"grouping_output\")\n",
    "mapping_output_ds = mapping_output_dir.as_dataset().parse_delimited_files()\n",
    "#This is to output the final file\n",
    "\n",
    "preprocess_output_dir = PipelineData(name=\"preprocess_output\", \n",
    "                      datastore=datastore_preprocess,output_path_on_compute=\"preprocess_output\")\n",
    "#This is to output the final file\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters for the pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue_length = PipelineParameter(name=\"queue_length\", default_value=5)\n",
    "num_node = PipelineParameter(name=\"num_node\", default_value=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define environments and processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment for mapping/distributing step\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "# create a new runconfig object\n",
    "mapping_run_config = RunConfiguration()\n",
    "\n",
    "# enable Docker \n",
    "mapping_run_config.environment.docker.enabled = True\n",
    "\n",
    "# set Docker base image to the default CPU-based image\n",
    "mapping_run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "mapping_run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# specify CondaDependencies obj\n",
    "mapping_run_config.environment.python.conda_dependencies = CondaDependencies.create(pip_packages=['azure-servicebus','azureml-defaults','pandas'])\n",
    "\n",
    "\n",
    "#Mapping/grouping step\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "distributing_step = PythonScriptStep(name=\"distributing_step\",\n",
    "   script_name=\"distributing.py\",\n",
    "   arguments=[\"--queue_length\", queue_length, \"--num_node\", num_node,\"--grouping_output\", grouping_output_ds],\n",
    "   outputs=[mapping_output_ds],\n",
    "   compute_target=compute_target,\n",
    "   source_directory=scripts_folder,\n",
    "   runconfig =mapping_run_config,\n",
    "    allow_reuse=False\n",
    ")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for preprocessing step\n",
    "from azureml.pipeline.core import PipelineParameter,StepSequence\n",
    "from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "from azureml.core import Environment\n",
    "\n",
    "preprocess_conda_deps = CondaDependencies.create(pip_packages=[\"azureml-defaults\",\"azureml-dataset-runtime[fuse,pandas]\",\"xlrd\",\"azure-servicebus\",'azure-storage-blob'])\n",
    "preprocess_env = Environment(name=\"preprocess_environment\")\n",
    "preprocess_env.python.conda_dependencies = preprocess_conda_deps\n",
    "preprocess_env.docker.enabled = True\n",
    "preprocess_env.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "\n",
    "\n",
    "preprocess_parallel_run_config = ParallelRunConfig(\n",
    "    source_directory=scripts_folder,\n",
    "    \n",
    "    entry_script=\"preprocessing.py\",\n",
    "    mini_batch_size=PipelineParameter(name=\"preprocess_batch_size_param\", default_value=\"1\"),\n",
    "\n",
    "    error_threshold=1,\n",
    "    output_action=\"summary_only\",\n",
    "    append_row_file_name=\"outputs.txt\",\n",
    "    environment=preprocess_env,\n",
    "    compute_target=compute_target,\n",
    "#     process_count_per_node=PipelineParameter(name=\"process_count_param\", default_value=2),\n",
    "    node_count=num_node)\n",
    "\n",
    "preprocess_parallelrun_step = ParallelRunStep(\n",
    "    name=\"preprocess\",\n",
    "    arguments=[\"--datastore_sourcefiles_dir\", datastore_sourcefiles_dir,\n",
    "               \"--preprocess_output_dir\", preprocess_output_dir],\n",
    "\n",
    "    parallel_run_config=preprocess_parallel_run_config,\n",
    "    side_inputs=[datastore_sourcefiles_dir],\n",
    "    inputs=[grouping_output_ds],\n",
    "    output=preprocess_output_dir,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pipeline and submit a run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step distributing_step [a6c5f12d][86af27ce-7450-4a8c-895d-ce10359e0b19], (This step will run and generate new outputs)Created step distributing_step [331602a9][97c12b09-2478-4c6b-8396-6109fb0dd28d], (This step will run and generate new outputs)\n",
      "\n",
      "Created step preprocess [5078ac33][b4dd53a4-d810-4452-926d-13f5e13a29bd], (This step will run and generate new outputs)\n",
      "Using data reference datastore_sourcefiles_dir_0 for StepId [af240703][f92e9235-3f6c-4b14-8c1c-0df1be1748fc], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Submitted PipelineRun bd166167-653f-490e-b291-72020522035c\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/preprocessing_example/runs/bd166167-653f-490e-b291-72020522035c?wsid=/subscriptions/0e9bace8-7a81-4922-83b5-d995ff706507/resourcegroups/azureml/workspaces/ws01ent\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[distributing_step, preprocess_parallelrun_step])\n",
    "experiment = Experiment(ws, 'preprocessing_example')\n",
    "pipeline_run = experiment.submit(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish the pipeline to use with external applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Name</th><th>Id</th><th>Status</th><th>Endpoint</th></tr><tr><td>preprocessing_example</td><td><a href=\"https://ml.azure.com/pipelines/7f233a74-c3fb-4736-a32c-90bbc7753cbf?wsid=/subscriptions/0e9bace8-7a81-4922-83b5-d995ff706507/resourcegroups/azureml/workspaces/ws01ent\" target=\"_blank\" rel=\"noopener\">7f233a74-c3fb-4736-a32c-90bbc7753cbf</a></td><td>Active</td><td><a href=\"https://westus2.api.azureml.ms/pipelines/v1.0/subscriptions/0e9bace8-7a81-4922-83b5-d995ff706507/resourceGroups/azureml/providers/Microsoft.MachineLearningServices/workspaces/ws01ent/PipelineRuns/PipelineSubmit/7f233a74-c3fb-4736-a32c-90bbc7753cbf\" target=\"_blank\" rel=\"noopener\">REST Endpoint</a></td></tr></table>"
      ],
      "text/plain": [
       "Pipeline(Name: preprocessing_example,\n",
       "Id: 7f233a74-c3fb-4736-a32c-90bbc7753cbf,\n",
       "Status: Active,\n",
       "Endpoint: https://westus2.api.azureml.ms/pipelines/v1.0/subscriptions/0e9bace8-7a81-4922-83b5-d995ff706507/resourceGroups/azureml/providers/Microsoft.MachineLearningServices/workspaces/ws01ent/PipelineRuns/PipelineSubmit/7f233a74-c3fb-4736-a32c-90bbc7753cbf)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_run.publish_pipeline(name =\"preprocessing_example\", description =\"preprocessing_example pipeline\", version=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
