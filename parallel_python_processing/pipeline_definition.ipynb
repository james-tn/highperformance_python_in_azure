{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logon to the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: ws01ent\n",
      "Azure region: westus2\n",
      "Subscription id: 0e9bace8-7a81-4922-83b5-d995ff706507\n",
      "Resource group: azureml\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "auth =InteractiveLoginAuthentication(tenant_id='72f988bf-86f1-41af-91ab-2d7cd011db47')\n",
    "from azureml.core import Workspace\n",
    "subscription_id = \"0e9bace8-7a81-4922-83b5-d995ff706507\" #you should be owner or contributor\n",
    "resource_group = \"azureml\" #you should be owner or contributor\n",
    "workspace_name = \"ws01ent\" #your workspace name\n",
    "workspace_region = \"westus2\" #your region\n",
    "ws = Workspace.create(name = workspace_name,\n",
    "                      subscription_id = subscription_id,\n",
    "                      resource_group = resource_group,\n",
    "                      location = workspace_region,\n",
    "                      auth=auth, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://adlsdatalakegen6.blob.core.windows.net/landing/source1/datafile1.csv\n",
      "https://adlsdatalakegen6.blob.core.windows.net/landing/source1/datafile2.csv\n",
      "https://adlsdatalakegen6.blob.core.windows.net/landing/source1/datafile3.csv\n",
      "https://adlsdatalakegen6.blob.core.windows.net/landing/source1/datafile4.csv\n",
      "https://adlsdatalakegen6.blob.core.windows.net/landing/source1/datafile5.csv\n",
      "https://adlsdatalakegen6.blob.core.windows.net/landing/source2/datafile2.csv\n",
      "https://adlsdatalakegen6.blob.core.windows.net/landing/source2/datafile3.csv\n",
      "https://adlsdatalakegen6.blob.core.windows.net/landing/source2/datafile4.csv\n",
      "https://adlsdatalakegen6.blob.core.windows.net/landing/source2/datafile1.csv\n",
      "https://adlsdatalakegen6.blob.core.windows.net/landing/source2/datafile5.csv\n"
     ]
    }
   ],
   "source": [
    "from azure.servicebus import QueueClient,ServiceBusClient\n",
    "import json\n",
    "import os\n",
    "keyvault = ws.get_default_keyvault()\n",
    "con_str =keyvault.get_secret('servicebusconstr')\n",
    "\n",
    "sb_client = ServiceBusClient.from_connection_string(con_str)\n",
    "queue_client = sb_client.get_queue(\"landing\")\n",
    "with queue_client.get_receiver(prefetch=1000) as queue_receiver:\n",
    "    messages = queue_receiver.fetch_next(timeout=30, max_batch_size=1000)\n",
    "    for message in messages:\n",
    "        json_content = json.loads(str(message))\n",
    "        url = json_content['data']['url']\n",
    "        folder_name = url.split(\"/\")[-2]\n",
    "        file_name = os.path.basename(url)\n",
    "#         message.complete()\n",
    "#         file_path=os.environ.get('AZUREML_DATAREFERENCE_datastore_sourcefiles_dir')+\"/\"+folder_name+\"/\"+file_name\n",
    "\n",
    "        print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "import joblib\n",
    "model_name=\"porto_seguro_safe_driver_model\"\n",
    "model = Model(ws, model_name)\n",
    "model.download(\"model\",exist_ok =True)\n",
    "model_path = os.path.join(\"model\", model_name)\n",
    "LGBM_MODEL = joblib.load(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.read_csv(\"C:\\\\Users\\\\janguy\\\\Downloads\\\\datafile3.csv\")\n",
    "test_df= test_df.drop(['id'], axis=1)\n",
    "output = LGBM_MODEL.predict(test_df)\n",
    "output = pd.DataFrame({\"score\":output, \"source\":[\"datafile1.csv\"]*len(output)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### Create or Attach existing compute resource for Python steps\n",
    "By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this tutorial, you create Azure Machine Learning Compute as your training environment. The code below creates the compute clusters for you if they don't already exist in your workspace.\n",
    "\n",
    "**Creation of compute takes approximately 5 minutes. If the AmlCompute with that name is already in your workspace the code will skip the creation process.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found compute target. just use it. worker-cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"worker-cpu\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 10)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D12_V2\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core import Keyvault\n",
    "import os\n",
    "\n",
    "keyvault = ws.get_default_keyvault()\n",
    "# keyvault.set_secret(name=\"servicebusconstr\", value = 'YOUR_SERVICE_BUS_CONNECTION_STRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This is to mount a BLob datastore\n",
    "from azureml.core import Datastore\n",
    "\n",
    "key =keyvault.get_secret('adlsgen6key')\n",
    "account_name = 'adlsdatalakegen6'\n",
    "datastore_sourcefiles= Datastore.register_azure_blob_container(workspace=ws, datastore_name = 'adlsgen6landing', \n",
    "                                                     container_name='landing',\n",
    "                                                     account_name= account_name, account_key=key,create_if_not_exists=False)\n",
    "\n",
    "datastore_batch_score= Datastore.register_azure_blob_container(workspace=ws, datastore_name = 'adlsgen6process', \n",
    "                                                     container_name='process',\n",
    "                                                     account_name= account_name, account_key=key,create_if_not_exists=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Intermediate/Output Data\n",
    "Intermediate data (or output of a Step) is represented by [PipelineData](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinedata?view=azure-ml-py) object. PipelineData can be produced by one step and consumed in another step by providing the PipelineData object as an output of one step and the input of one or more steps.\n",
    "\n",
    "**Constructing PipelineData**\n",
    "- name: [Required] Name of the data item within the pipeline graph\n",
    "- datastore_name: Name of the Datastore to write this output to\n",
    "- output_name: Name of the output\n",
    "- output_mode: Specifies \"upload\" or \"mount\" modes for producing output (default: mount)\n",
    "- output_path_on_compute: For \"upload\" mode, the path to which the module writes this output during execution\n",
    "- output_overwrite: Flag to overwrite pre-existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline, PipelineData,PipelineParameter\n",
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "\n",
    "\n",
    "scripts_folder = \"scripts\"\n",
    "#This is the source directory for the landing folder. It's used to read from files from\n",
    "datastore_sourcefiles_dir = Dataset.File.from_files((datastore_sourcefiles, \"/\")).as_named_input(\"datastore_sourcefiles_dir\")\n",
    "\n",
    "#This is to output the mapping file (mapping.csv)\n",
    "mapping_output_dir = PipelineData(name=\"grouping_output\", \n",
    "                          datastore=datastore_batch_score, \n",
    "                          output_path_on_compute=\"grouping_output\")\n",
    "mapping_output_ds = mapping_output_dir.as_dataset()\n",
    "\n",
    "#This is to output the final file\n",
    "\n",
    "batch_score_output_dir = PipelineData(name=\"batch_score_output\", \n",
    "                      datastore=datastore_batch_score,output_path_on_compute=\"batch_score_output\")\n",
    "#This is to output the final file\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define parameters for the pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue_length = PipelineParameter(name=\"queue_length\", default_value=5)\n",
    "num_node = PipelineParameter(name=\"num_node\", default_value=2)\n",
    "core_per_node = PipelineParameter(name=\"core_per_node\", default_value=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define environments and processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment for mapping/distributing step\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "# create a new runconfig object\n",
    "mapping_run_config = RunConfiguration()\n",
    "\n",
    "# enable Docker \n",
    "mapping_run_config.environment.docker.enabled = True\n",
    "\n",
    "# set Docker base image to the default CPU-based image\n",
    "mapping_run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "mapping_run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# specify CondaDependencies obj\n",
    "mapping_run_config.environment.python.conda_dependencies = CondaDependencies.create(pip_packages=['azure-servicebus','azureml-defaults','pandas'])\n",
    "\n",
    "\n",
    "#Mapping/grouping step\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "distributing_step = PythonScriptStep(name=\"distributing_step\",\n",
    "   script_name=\"distributing.py\",\n",
    "   arguments=[\"--queue_length\", queue_length, \"--num_node\", num_node, \"--core_per_node\", core_per_node,\"--grouping_output\", mapping_output_ds],\n",
    "   outputs=[mapping_output_ds],\n",
    "   compute_target=compute_target,\n",
    "   source_directory=scripts_folder,\n",
    "   runconfig =mapping_run_config,\n",
    "    allow_reuse=False\n",
    ")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_scoreing step\n",
    "from azureml.pipeline.core import PipelineParameter,StepSequence\n",
    "from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "from azureml.core import Environment\n",
    "\n",
    "batch_score_conda_deps = CondaDependencies.create(conda_packages=['lightgbm'], pip_packages=[\"azureml-defaults\",\"azureml-dataset-runtime[fuse,pandas]\"\n",
    "                                                               ,\"azure-servicebus\",'azure-storage-blob',\n",
    "                                                               'joblib'])\n",
    "batch_score_env = Environment(name=\"batch_score_environment\")\n",
    "batch_score_env.python.conda_dependencies = batch_score_conda_deps\n",
    "batch_score_env.docker.enabled = True\n",
    "batch_score_env.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "\n",
    "\n",
    "batch_score_parallel_run_config = ParallelRunConfig(\n",
    "    source_directory=scripts_folder,\n",
    "    \n",
    "    entry_script=\"batch_score.py\",\n",
    "    mini_batch_size=PipelineParameter(name=\"batch_score_batch_size_param\", default_value=\"1\"),\n",
    "\n",
    "    error_threshold=1,\n",
    "    output_action=\"append_row\",\n",
    "    append_row_file_name=\"outputs.txt\",\n",
    "    environment=batch_score_env,\n",
    "    compute_target=compute_target,\n",
    "    process_count_per_node=PipelineParameter(name=\"process_count_param\", default_value=6),\n",
    "    node_count=num_node)\n",
    "# process_count_per_node should be set to the number of cores per node or higher to maximize parallelism\n",
    "batch_score_parallelrun_step = ParallelRunStep(\n",
    "    name=\"batchscore\",\n",
    "    arguments=[\"--datastore_sourcefiles_dir\", datastore_sourcefiles_dir,\n",
    "               \"--batch_score_output_dir\", batch_score_output_dir],\n",
    "\n",
    "    parallel_run_config=batch_score_parallel_run_config,\n",
    "    side_inputs=[datastore_sourcefiles_dir],\n",
    "    inputs=[mapping_output_ds],\n",
    "    output=batch_score_output_dir,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define pipeline and submit a run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step distributing_step [ada8ffb0][0defa60e-fb6a-4e0d-8b7e-6f84aefd769c], (This step will run and generate new outputs)Created step batchscore [9b505289][4044c18b-c5ad-4c9c-be77-a0d173ce15d7], (This step will run and generate new outputs)\n",
      "\n",
      "Using data reference datastore_sourcefiles_dir_0 for StepId [886a3da7][f92e9235-3f6c-4b14-8c1c-0df1be1748fc], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Submitted PipelineRun 4239a1ed-4506-49d6-bbad-4f59235455af\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/preprocessing_example/runs/4239a1ed-4506-49d6-bbad-4f59235455af?wsid=/subscriptions/0e9bace8-7a81-4922-83b5-d995ff706507/resourcegroups/azureml/workspaces/ws01ent\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[distributing_step, batch_score_parallelrun_step])\n",
    "experiment = Experiment(ws, 'preprocessing_example')\n",
    "pipeline_run = experiment.submit(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2666704235b4358beb93d10b2213423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Publish the pipeline to use with external applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Name</th><th>Id</th><th>Status</th><th>Endpoint</th></tr><tr><td>preprocessing_example</td><td><a href=\"https://ml.azure.com/pipelines/56ee21ae-db27-4501-83c0-59ca70a0b9bb?wsid=/subscriptions/0e9bace8-7a81-4922-83b5-d995ff706507/resourcegroups/azureml/workspaces/ws01ent\" target=\"_blank\" rel=\"noopener\">56ee21ae-db27-4501-83c0-59ca70a0b9bb</a></td><td>Active</td><td><a href=\"https://westus2.api.azureml.ms/pipelines/v1.0/subscriptions/0e9bace8-7a81-4922-83b5-d995ff706507/resourceGroups/azureml/providers/Microsoft.MachineLearningServices/workspaces/ws01ent/PipelineRuns/PipelineSubmit/56ee21ae-db27-4501-83c0-59ca70a0b9bb\" target=\"_blank\" rel=\"noopener\">REST Endpoint</a></td></tr></table>"
      ],
      "text/plain": [
       "Pipeline(Name: preprocessing_example,\n",
       "Id: 56ee21ae-db27-4501-83c0-59ca70a0b9bb,\n",
       "Status: Active,\n",
       "Endpoint: https://westus2.api.azureml.ms/pipelines/v1.0/subscriptions/0e9bace8-7a81-4922-83b5-d995ff706507/resourceGroups/azureml/providers/Microsoft.MachineLearningServices/workspaces/ws01ent/PipelineRuns/PipelineSubmit/56ee21ae-db27-4501-83c0-59ca70a0b9bb)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_run.publish_pipeline(name =\"preprocessing_example\", description =\"preprocessing_example pipeline\", version=1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
