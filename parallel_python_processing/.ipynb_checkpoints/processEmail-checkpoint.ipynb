{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws)\n",
    "\n",
    "{\n",
    "    \"subscription_id\": \"17a92c38-4328-4a79-b356-bc50e119219f\",\n",
    "    \"resource_group\": \"rg-e-idc-ml-batch\",\n",
    "    \"workspace_name\": \"ml-e-idc-batch\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing interactive authentication. Please follow the instructions on the terminal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Note, we have launched a browser for you to login. For old experience with device code, use \"az login --use-device-code\"\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "auth =InteractiveLoginAuthentication()\n",
    "from azureml.core import Workspace\n",
    "subscription_id = \"17a92c38-4328-4a79-b356-bc50e119219f\" #you should be owner or contributor\n",
    "resource_group = \"rg-e-idc-ml-batch\" #you should be owner or contributor\n",
    "workspace_name = \"ml-e-idc-batch\" #your workspace name\n",
    "workspace_region = \"westus2\" #your region\n",
    "ws = Workspace.create(name = workspace_name,\n",
    "                      subscription_id = subscription_id,\n",
    "                      resource_group = resource_group,\n",
    "                      location = workspace_region,\n",
    "                      auth=auth, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"batchcluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 1)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "from azureml.core import Workspace\n",
    "\n",
    "image_datastore = Datastore.register_azure_blob_container(workspace=ws,\n",
    "                                                         datastore_name=\"storage_images\",\n",
    "                                                         container_name=\"images\",\n",
    "                                                         account_name=\"saeidcfnc01images\",\n",
    "                                                         account_key=\"HuKo1e+ii++fGdF0DAuHrJNEFnZS7v+Qh2Ympq27aVugAJ6GSe57pjBDh1x8vFTvbprFHz5kGJxuGUbPbzVx+A==\")\n",
    "print(image_datastore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "path_on_datastore = image_datastore.path('images/')\n",
    "input_images_ds = Dataset.File.from_files(path=path_on_datastore, validate=False)\n",
    "\n",
    "print(input_images_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\n",
    "from azureml.pipeline.core import PipelineParameter\n",
    "\n",
    "pipeline_param = PipelineParameter(name=\"images_param\", default_value=input_images_ds)\n",
    "input_images_ds_consumption = DatasetConsumptionConfig(\"images_param_config\", pipeline_param).as_mount()\n",
    "\n",
    "print(input_images_ds_consumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "\n",
    "output_dir = PipelineData(name=\"inferences\", datastore=image_datastore)\n",
    "print(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline, PipelineData,PipelineParameter\n",
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "scripts_folder = \"scripts\"\n",
    "queue_length = PipelineParameter(name=\"queue_length\", default_value=5)\n",
    "num_node = PipelineParameter(name=\"num_node\", default_value=2)\n",
    "core_per_node = PipelineParameter(name=\"core_per_node\", default_value=3)\n",
    "\n",
    "#This is the source directory for the landing folder. It's used to read from files from\n",
    "#datastore_sourcefiles_dir = Dataset.File.from_files((image_datastore, \"/\")).as_named_input(\"datastore_sourcefiles_dir\")\n",
    "\n",
    "\n",
    "#This is to output the mapping file (mapping.csv)\n",
    "mapping_output_dir = PipelineData(name=\"grouping_output\", \n",
    "                          datastore=image_datastore, \n",
    "                          output_path_on_compute=\"grouping_output\")\n",
    "mapping_output_ds = mapping_output_dir.as_dataset().parse_delimited_files()\n",
    "#This is to output the final file\n",
    "\n",
    "preprocess_output_dir = PipelineData(name=\"preprocess_output\", \n",
    "                      datastore=image_datastore,output_path_on_compute=\"preprocess_output\")\n",
    "#This is to output the final file\n",
    "\n",
    "print(mapping_output_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Environment for mapping/distributing step\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "# create a new runconfig object\n",
    "mapping_run_config = RunConfiguration()\n",
    "\n",
    "# enable Docker \n",
    "mapping_run_config.environment.docker.enabled = True\n",
    "\n",
    "# set Docker base image to the default CPU-based image\n",
    "mapping_run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "mapping_run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# specify CondaDependencies obj\n",
    "mapping_run_config.environment.python.conda_dependencies = CondaDependencies.create(pip_packages=['azure-servicebus','azureml-defaults','pandas','redis'])\n",
    "\n",
    "\n",
    "#Mapping/grouping step\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "distributing_step = PythonScriptStep(name=\"distributing_step\",\n",
    "   script_name=\"distributing.py\",\n",
    "   arguments=[\"--queue_length\", queue_length, \"--num_node\", num_node, \"--core_per_node\", core_per_node,\"--grouping_output\", mapping_output_ds],\n",
    "   outputs=[mapping_output_ds],\n",
    "   compute_target=compute_target,\n",
    "   source_directory=scripts_folder,\n",
    "   runconfig =mapping_run_config,\n",
    "    allow_reuse=False\n",
    ")\n",
    "\n",
    "print(distributing_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# for preprocessing step\n",
    "from azureml.pipeline.core import PipelineParameter,StepSequence\n",
    "from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "from azureml.core import Environment\n",
    "\n",
    "preprocess_conda_deps = CondaDependencies.create(pip_packages=[\"azureml-defaults\",\"azureml-dataset-runtime[fuse,pandas]\",\"xlrd\",\"azure-servicebus\",\"azure-storage-blob\",\"redis\"])\n",
    "preprocess_env = Environment(name=\"preprocess_environment\")\n",
    "preprocess_env.python.conda_dependencies = preprocess_conda_deps\n",
    "preprocess_env.docker.enabled = True\n",
    "preprocess_env.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "\n",
    "\n",
    "preprocess_parallel_run_config = ParallelRunConfig(\n",
    "    source_directory=scripts_folder,\n",
    "    \n",
    "    entry_script=\"preprocessing.py\",\n",
    "    mini_batch_size=PipelineParameter(name=\"preprocess_batch_size_param\", default_value=\"1\"),\n",
    "\n",
    "    error_threshold=1,\n",
    "    output_action=\"summary_only\",\n",
    "    append_row_file_name=\"outputs.txt\",\n",
    "    environment=preprocess_env,\n",
    "    compute_target=compute_target,\n",
    "    process_count_per_node=PipelineParameter(name=\"process_count_param\", default_value=2),\n",
    "    node_count=num_node)\n",
    "# process_count_per_node should be set to the number of cores per node or higher to maximize parallelism\n",
    "preprocess_parallelrun_step = ParallelRunStep(\n",
    "    name=\"preprocess\",\n",
    "   #arguments=[\"--datastore_sourcefiles_dir\", datastore_sourcefiles_dir,\n",
    "   arguments=[\"--preprocess_output_dir\", preprocess_output_dir],\n",
    "\n",
    "    parallel_run_config=preprocess_parallel_run_config,\n",
    "    #side_inputs=[datastore_sourcefiles_dir],\n",
    "    inputs=[mapping_output_ds],\n",
    "    output=preprocess_output_dir,\n",
    "    allow_reuse=False\n",
    ")\n",
    "\n",
    "print(preprocess_parallelrun_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[distributing_step, preprocess_parallelrun_step])\n",
    "experiment = Experiment(ws, 'preprocessing_example')\n",
    "pipeline_run = experiment.submit(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "pipeline_run.publish_pipeline(name =\"preprocessing_example\", description =\"preprocessing_example pipeline\", version=1.0)\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "PyCharm (generic_research)",
   "language": "python",
   "name": "pycharm-b34a94c6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
